{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af59f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "import utils\n",
    "# from utils import load_data, get_train_val_split, get_stratified_splitter\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedGroupKFold, train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from scipy.stats import kstest, kruskal, mannwhitneyu\n",
    "from scipy.special import softmax \n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import re\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "\n",
    "import optuna\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f663bf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/Users/sharaddargan/Documents/Course Content/Spring 2023/Probability and Statistics 2/Project/EnergyPrediction-ASHRAE/code/utils.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81c2569b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.07 MB\n",
      "Memory usage after optimization is: 0.02 MB\n",
      "Decreased by 73.88%\n",
      "Memory usage of dataframe is 9.60 MB\n",
      "Memory usage after optimization is: 3.07 MB\n",
      "Decreased by 68.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sharaddargan/Documents/Course Content/Spring 2023/Probability and Statistics 2/Project/EnergyPrediction-ASHRAE/code/utils.py:143: FutureWarning: Using .astype to convert from timezone-aware dtype to timezone-naive dtype is deprecated and will raise in a future version.  Use obj.tz_localize(None) or obj.tz_convert('UTC').tz_localize(None) instead\n",
      "  weather_train['timestamp'] = pd.to_datetime(weather_train['timestamp'], infer_datetime_format = True, utc = True).astype('datetime64[ns]')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 19.04 MB\n",
      "Memory usage after optimization is: 5.13 MB\n",
      "Decreased by 73.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sharaddargan/Documents/Course Content/Spring 2023/Probability and Statistics 2/Project/EnergyPrediction-ASHRAE/code/utils.py:151: FutureWarning: Using .astype to convert from timezone-aware dtype to timezone-naive dtype is deprecated and will raise in a future version.  Use obj.tz_localize(None) or obj.tz_convert('UTC').tz_localize(None) instead\n",
      "  train['timestamp'] = pd.to_datetime(train['timestamp'], infer_datetime_format = True, utc = True).astype('datetime64[ns]')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 616.95 MB\n",
      "Memory usage after optimization is: 289.19 MB\n",
      "Decreased by 53.12%\n",
      "Memory usage of dataframe is 1272.51 MB\n",
      "Memory usage after optimization is: 358.53 MB\n",
      "Decreased by 71.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sharaddargan/Documents/Course Content/Spring 2023/Probability and Statistics 2/Project/EnergyPrediction-ASHRAE/code/utils.py:166: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[(train['site_id'] == 0) & (train['meter'] == 0)]['meter_reading'] = 0.2931 * train[(train['site_id'] == 0) & (train['meter'] == 0)]['meter_reading']\n"
     ]
    }
   ],
   "source": [
    "data_dict = utils.load_data('ashrae-energy-prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aea63a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add weather features \n",
    "weather_features = ['cloud_coverage', 'dew_temperature', 'air_temperature', \n",
    "                    'sea_level_pressure', 'wind_direction', 'wind_speed', 'precip_depth_1_hr',]\n",
    "\n",
    "hourly_by_site = data_dict[\"X_train\"].groupby(['hour', 'month', 'site_id'])[weather_features].mean().reset_index()\n",
    "\n",
    "data_dict[\"X_train\"] = data_dict[\"X_train\"].merge(\n",
    "    hourly_by_site, \n",
    "    on=['hour', 'month', 'site_id'], \n",
    "    how='left', \n",
    "    suffixes=(None, '_hourly_by_site')\n",
    ")\n",
    "\n",
    "del hourly_by_site\n",
    "\n",
    "for feature in weather_features:\n",
    "    # Fill in NA values from weather with hourly by site columns \n",
    "    data_dict[\"X_train\"][feature].fillna(\n",
    "        data_dict[\"X_train\"][feature + \"_hourly_by_site\"],\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    # Fill in the rest with the median \n",
    "    data_dict[\"X_train\"][feature].fillna(\n",
    "        data_dict[\"X_train\"][feature].median(),\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    data_dict[\"X_train\"][feature + \"_diff_hourly_from_mean\"] = data_dict[\"X_train\"][feature] - \\\n",
    "        data_dict[\"X_train\"][feature + \"_hourly_by_site\"]\n",
    "    \n",
    "data_dict[\"X_train\"] = data_dict[\"X_train\"].drop(columns = [feat + \"_hourly_by_site\" for feat in weather_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1ad44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in NA with median values for floor count and year_built\n",
    "for feature in ['year_built', 'floor_count']:\n",
    "    data_dict[\"X_train\"][feature].fillna(\n",
    "        data_dict[\"X_train\"][feature].median(), \n",
    "        inplace=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "557c72c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['weather_test', 'X_train', 'X_test', 'y_train'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865a4187",
   "metadata": {},
   "source": [
    "## Examine Differences (Non-Parametric)\n",
    "Using Bonferonni's Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d409d8",
   "metadata": {},
   "source": [
    "### Milestone 2. \n",
    "- Show difference in sites across meter readings \n",
    "- get average meter reading per day per site \n",
    "- conduct a (non-parametric ANOVA) KS OR pairs (mann-whitney) to show that they are diff \n",
    "- train a model per site id (with rudimentary hyperparameter tuning) \n",
    "- John sites 0-7, Sharad sites 8-15 \n",
    "\n",
    "### Milestone 3. Determine, per site, which primary uses are similar (if they have only a few buildings), which are diff\n",
    "- for a given primary use, if diff, identify \"clusters\" of buildings that are similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86eb2613",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['year_built', 'floor_count', 'air_temperature',\n",
    "       'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr',\n",
    "       'sea_level_pressure', 'wind_direction', 'wind_speed',\n",
    "       'air_temperature_mean_lag7', 'air_temperature_max_lag7',\n",
    "       'air_temperature_min_lag7', 'air_temperature_std_lag7',\n",
    "       'cloud_coverage_mean_lag7', 'cloud_coverage_max_lag7',\n",
    "       'cloud_coverage_min_lag7', 'cloud_coverage_std_lag7',\n",
    "       'dew_temperature_mean_lag7', 'dew_temperature_max_lag7',\n",
    "       'dew_temperature_min_lag7', 'dew_temperature_std_lag7',\n",
    "       'precip_depth_1_hr_mean_lag7', 'precip_depth_1_hr_max_lag7',\n",
    "       'precip_depth_1_hr_min_lag7', 'precip_depth_1_hr_std_lag7',\n",
    "       'sea_level_pressure_mean_lag7', 'sea_level_pressure_max_lag7',\n",
    "       'sea_level_pressure_min_lag7', 'sea_level_pressure_std_lag7',\n",
    "       'wind_direction_mean_lag7', 'wind_direction_max_lag7',\n",
    "       'wind_direction_min_lag7', 'wind_direction_std_lag7',\n",
    "       'wind_speed_mean_lag7', 'wind_speed_max_lag7', 'wind_speed_min_lag7',\n",
    "       'wind_speed_std_lag7', 'log_square_feet', 'weekday', 'hour', 'day',\n",
    "       'weekend', 'month', 'primary_use_enc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4d296e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building_cluster_mapping = pd.read_csv(\"building_cluster_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c21af1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_dict['X_train'].loc[:, features+['meter', 'site_id', 'building_id']], data_dict['y_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dca43ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = X.reset_index().merge(building_cluster_mapping, on=[\"building_id\", \"meter\"], how=\"left\").set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44645aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/conda-env/lib/python3.10/site-packages/sklearn/model_selection/_split.py:909: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "splitter_gen = utils.get_stratified_splitter(X, y)\n",
    "train_index, test_index = next(splitter_gen)\n",
    "X_train, y_train, X_test, y_test = X.loc[train_index, :], y[train_index], X.loc[test_index, :], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04453a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./models/v4/\"\n",
    "models = defaultdict(list)\n",
    "for m in os.listdir(model_dir):\n",
    "    if '.pkl' in m:\n",
    "        with open(model_dir + m, 'rb') as f:\n",
    "            models[int(m.split(\"_\")[1])].append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2031449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved as \"site_meter\"\n",
    "class WeightedEnsembleRegressor(BaseEstimator, RegressorMixin):\n",
    "    # Store weights per site per meter\n",
    "    def __init__(self, models: dict, chunk_size: int = 1000000):\n",
    "        self.models = models\n",
    "        self.weights = {}\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "        for i in range(4):\n",
    "            self.weights[i] = np.ones(len(self.models[i])) / len(self.models[i])\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        new_weights = self.weights.copy()\n",
    "        \n",
    "        for i in range(4): \n",
    "            new_weights[i] = np.zeros(len(self.weights[i]))\n",
    "        \n",
    "        for chunk in range(0, len(X), self.chunk_size):\n",
    "            for meter in X['meter'].unique():\n",
    "\n",
    "                for i in range(len(self.models[meter])):\n",
    "                    try:\n",
    "                        prediction = self.models[meter][i].predict(\n",
    "                            X.loc[chunk: chunk + self.chunk_size][X['meter'] == meter] \\\n",
    "                                .drop(['meter', 'site_id', 'building_id'], axis=1)\n",
    "                        )\n",
    "                        new_weights[meter][i] += mean_squared_error(\n",
    "                            prediction, \n",
    "                            y.iloc[chunk: chunk + self.chunk_size][X['meter'] == meter]\n",
    "                        )\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "        for i in range(4): \n",
    "            self.weights[i] = softmax(-new_weights[i])\n",
    "            \n",
    "    \n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(len(X))\n",
    "        for chunk in range(0, len(X), self.chunk_size):\n",
    "            for meter in X['meter'].unique():\n",
    "                for i in range(len(self.models[meter])):\n",
    "                    prediction = self.models[meter][i].predict(\n",
    "                    X.iloc[chunk: chunk + self.chunk_size][X['meter'] == meter] \\\n",
    "                        .drop(['meter', 'site_id', 'building_id'], axis=1)\n",
    "                    )\n",
    "                    preds[chunk: chunk + self.chunk_size][X['meter'] == meter] += self.weights[meter][i] * prediction\n",
    "  \n",
    "                    \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04b0d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved as \"site_meter\"\n",
    "class WeightedEnsembleRegressor(BaseEstimator, RegressorMixin):\n",
    "    # Store weights per site per meter\n",
    "    def __init__(self, models: dict, chunk_size: int = 1000000):\n",
    "        self.models = models\n",
    "        self.weights = {}\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "        for i in range(4):\n",
    "            self.weights[i] = np.ones(len(self.models[i])) / len(self.models[i])\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        new_weights = self.weights.copy()\n",
    "        \n",
    "        for i in range(4): \n",
    "            new_weights[i] = np.zeros(len(self.weights[i]))\n",
    "        \n",
    "        for chunk in range(0, len(X), self.chunk_size):\n",
    "            for meter in X['meter'].unique():\n",
    "\n",
    "                for i in range(len(self.models[meter])):\n",
    "                    try:\n",
    "                        prediction = self.models[meter][i].predict(\n",
    "                            X.loc[chunk: chunk + self.chunk_size,:].loc[X['meter'] == meter, :] \\\n",
    "                                .drop(['meter', 'site_id', 'building_id'], axis=1)\n",
    "                        )\n",
    "                        new_weights[meter][i] += mean_squared_error(\n",
    "                            prediction, \n",
    "                            y.iloc[chunk: chunk + self.chunk_size][X['meter'] == meter]\n",
    "                        )\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "        for i in range(4): \n",
    "            self.weights[i] = softmax(-new_weights[i])\n",
    "            \n",
    "    \n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(len(X))\n",
    "        for meter in X['meter'].unique():\n",
    "            for i in range(len(self.models[meter])):\n",
    "                prediction = self.models[meter][i].predict(\n",
    "                X.loc[X['meter'] == meter, :].drop(['meter', 'site_id', 'building_id'], axis=1)\n",
    "                )\n",
    "                preds[X['meter'] == meter] += self.weights[meter][i] * prediction\n",
    "                    \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d299cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w = WeightedEnsembleRegressor(models)\n",
    "w.fit(X_train.reset_index(drop=True), y_train.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d151be8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    preds = np.zeros(len(X))\n",
    "    for meter in X['meter'].unique():\n",
    "        for i in range(len(self.models[meter])):\n",
    "            prediction = self.models[meter][i].predict(\n",
    "            X.loc[X['meter'] == meter, :].drop(['meter', 'site_id', 'building_id'], axis=1)\n",
    "            )\n",
    "            preds[X['meter'] == meter] += self.weights[meter][i] * prediction\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2eaebef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7918102143022188"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_train.reset_index(drop=True), predict(w, X_train.reset_index(drop=True)), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d305518e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8038478151090174"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test.reset_index(drop=True), predict(w, X_test.reset_index(drop=True)), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c637ac82",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_df = predict_using_individual_estimators(X_test, y_test, estimators_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = predict_using_individual_estimators(X_train, y_train, estimators_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
